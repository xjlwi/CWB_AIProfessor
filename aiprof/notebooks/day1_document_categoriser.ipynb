{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import os\n",
    "\n",
    "import plotly as plt\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import spacy\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import regex as re\n",
    "import requests\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/01_raw/bbc/'\n",
    "business, entertainment, politics, sport, tech = [], [], [], [], []\n",
    "for file in os.listdir(data_dir + '/business/'):\n",
    "    with open(data_dir + '/business/' + file, 'r') as f:\n",
    "        business.append(f.read())\n",
    "        \n",
    "for file in os.listdir(data_dir + '/entertainment/'):\n",
    "    with open(data_dir + '/entertainment/' + file, 'r') as e:\n",
    "        entertainment.append(e.read())\n",
    "        \n",
    "for file in os.listdir(data_dir + '/politics/'):\n",
    "    with open(data_dir + '/politics/' + file, 'r') as e:\n",
    "        politics.append(e.read())\n",
    "                \n",
    "for file in os.listdir(data_dir + '/sport/'):\n",
    "    with open(data_dir + '/sport/' + file, 'r') as e:\n",
    "        sport.append(e.read())\n",
    "\n",
    "for file in os.listdir(data_dir + '/tech/'):\n",
    "    with open(data_dir + '/tech/' + file, 'r') as e:\n",
    "        tech.append(e.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of businssess 510, \n",
      " length of entertainment 386 \n",
      " length politics: 417, length sport 511, \n",
      " length tech 401\n"
     ]
    }
   ],
   "source": [
    "print ('Length of businssess {}, \\n length of entertainment {} \\n length politics: {}, length sport {}, \\n length tech {}'.format(len(business), len(entertainment), len(politics), len(sport), len(tech)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(x):\n",
    " tokenizer = RegexpTokenizer(r'\\w+')\n",
    " return tokenizer.tokenize(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Create an empty model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Add the TextCategorizer to the empty model\n",
    "textcat = nlp.add_pipe(\"textcat\")\n",
    "# textcat = nlp.remove_pipe('lemmatizer')\n",
    "# Add labels to text classifier\n",
    "textcat.add_label(\"business\")\n",
    "textcat.add_label(\"tech\")\n",
    "textcat.add_label(\"politics\")\n",
    "textcat.add_label(\"entertainment\")\n",
    "textcat.add_label(\"sport\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Text Categorizer Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_df  = pd.DataFrame({'text': business})\n",
    "business_df.insert(0, 'label', 'business', True)\n",
    "entertainment_df  = pd.DataFrame({'text': entertainment})\n",
    "entertainment_df.insert(0, 'label', 'entertainment')\n",
    "politics_df  = pd.DataFrame({'text': politics})\n",
    "politics_df.insert(0, 'label', 'politics')\n",
    "sport_df = pd.DataFrame({'text': sport})\n",
    "sport_df.insert(0, 'label', 'sport')\n",
    "tech_df = pd.DataFrame({'text': tech})\n",
    "tech_df.insert(0, 'label', 'tech')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = pd.concat([business_df, politics_df, sport_df, tech_df, entertainment_df])\n",
    "all_texts.to_csv('../data/02_intermediate/all_text_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>Ad sales boost Time Warner profit\\n\\nQuarterly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>Dollar gains on Greenspan speech\\n\\nThe dollar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>Yukos unit buyer faces loan claim\\n\\nThe owner...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>High fuel prices hit BA's profits\\n\\nBritish A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>Pernod takeover talk lifts Domecq\\n\\nShares in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "0  business  Ad sales boost Time Warner profit\\n\\nQuarterly...\n",
       "1  business  Dollar gains on Greenspan speech\\n\\nThe dollar...\n",
       "2  business  Yukos unit buyer faces loan claim\\n\\nThe owner...\n",
       "3  business  High fuel prices hit BA's profits\\n\\nBritish A...\n",
       "4  business  Pernod takeover talk lifts Domecq\\n\\nShares in..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_texts = pd.read_csv('../data/02_intermediate/all_text_labels.csv')\n",
    "all_texts = all_texts.drop(['Unnamed: 0'], axis=1)\n",
    "all_texts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = all_texts.drop('label',axis=1)\n",
    "y = all_texts['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "717     politics\n",
       "798     politics\n",
       "1330       sport\n",
       "18      business\n",
       "1391       sport\n",
       "          ...   \n",
       "296     business\n",
       "1701        tech\n",
       "781     politics\n",
       "1359       sport\n",
       "1836        tech\n",
       "Name: label, Length: 668, dtype: object"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = X_train['text'].values\n",
    "train_labels = [{'cats': {'business': label == 'business',\n",
    "                          'entertainment':label == 'entertainment',\n",
    "                          'politics' : label == 'politics',\n",
    "                          'sport': label == 'sport',\n",
    "                          'tech': label == 'tech'}} for label in y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Intel unveils laser breakthrough\\n\\nIntel has said it has found a way to put a silicon-based laser on a chip, raising hopes of much faster networks.\\n\\nScientists at Intel have overcome a fundamental problem that before now has prevented silicon being used to generate and amplify laser light. The breakthrough should make it easier to interconnect data networks with the chips that process the information. The Intel researchers said products exploiting the breakthrough should appear by the end of the decade.\\n\\n\"We\\'ve overcome a fundamental limit,\" said Dr Mario Paniccia, director of Intel\\'s photonics technology lab. Writing in the journal Nature, Dr Paniccia - and colleagues Haisheng Rong, Richard Jones, Ansheng Liu, Oded Cohen, Dani Hak and Alexander Fang - show how they have made a continuous laser from the same material used to make computer processors. Currently, says Dr Paniccia, telecommunications equipment that amplifies the laser light that travels down fibre optic cables is very expensive because of the exotic materials, such as gallium arsenide, used to make it.\\n\\nTelecommunications firms and chip makers would prefer to use silicon for these light-moving elements because it is cheap and many of the problems of using it in high-volume manufacturing have been solved. \"We\\'re trying to take our silicon competency in manufacturing and apply it to new areas,\" said Dr Paniccia. While work has been done to make some of the components that can move light around, before now silicon has not successfully been used to generate or amplify the laser light pulses used to send data over long distances. This is despite the fact that silicon is a much better amplifier of light pulses than the form of the material used in fibre optic cables. This improved amplification is due to the crystalline structure of the silicon used to make computer chips. Dr Paniccia said that the structure of silicon meant that when laser light passed through it, some colliding photons rip electrons off the atoms within the material. \"It creates a cloud of electrons sitting in the silicon and that absorbs all the light,\" he said. But the Intel researchers have found a way to suck away these errant electrons and turn silicon into a material that can both generate and amplify laser light. Even better, the laser light produced in this way can, with the help of easy-to-make filters, be tuned across a very wide range of frequencies. Semi-conductor lasers made before now have only produced light in a narrow frequency ranges. The result could be the close integration of the fibre optic cables that carry data as light with the computer chips that process it. Dr Paniccia said the work was the one of several steps needed if silicon was to be used to make components that could carry and process light in the form of data pulses. \"It\\'s a technical validation that it can work,\" he said.\\n',\n",
       " {'cats': {'business': False,\n",
       "   'entertainment': False,\n",
       "   'politics': False,\n",
       "   'sport': False,\n",
       "   'tech': True}})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine the texts and labels into a single list.\n",
    "train_data = list(zip(train_texts, train_labels))\n",
    "train_data[-3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create an optimizer using `nlp.begin_training()`. spaCy uses this optimizer to update the model. In general it's more efficient to train models in small batches. spaCy provides the minibatch function that returns a generator <u>yielding minibatches</u> for training. Finally, the minibatches are split into texts and labels, then used with nlp.update to update the model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.util import minibatch\n",
    "from spacy.training.example import Example\n",
    "\n",
    "spacy.util.fix_random_seed(1)\n",
    "optimizer = nlp.begin_training()\n",
    "\n",
    "# Create the batch generator with batch size = 8\n",
    "batches = minibatch(train_data, size=8)\n",
    "# Iterate through minibatches\n",
    "for batch in batches:\n",
    "    # Each batch is a list of (text, label) \n",
    "    for text, labels in batch:\n",
    "        doc = nlp.make_doc(text)\n",
    "        example = Example.from_dict(doc, labels)\n",
    "        nlp.update([example], sgd=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is using loop to reshuffle training data at the beginning of each loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'textcat': 568.5723469678775}\n",
      "{'textcat': 719.8168045304549}\n",
      "{'textcat': 788.481938169019}\n",
      "{'textcat': 836.7500243831912}\n",
      "{'textcat': 866.7167715687638}\n",
      "{'textcat': 896.268980501044}\n",
      "{'textcat': 913.9506380312018}\n",
      "{'textcat': 931.9563007129077}\n",
      "{'textcat': 949.2734642733325}\n",
      "{'textcat': 955.4252219810468}\n"
     ]
    }
   ],
   "source": [
    "from spacy.util import minibatch\n",
    "from spacy.training.example import Example\n",
    "import random\n",
    "\n",
    "random.seed(1)\n",
    "spacy.util.fix_random_seed(1)\n",
    "optimizer = nlp.begin_training()\n",
    "\n",
    "# Create the batch generator with batch size = 8\n",
    "batches = minibatch(train_data, size=8)\n",
    "\n",
    "losses = {}\n",
    "for epoch in range(10):\n",
    "    random.shuffle(train_data)\n",
    "    # Create the batch generator with batch size = 8\n",
    "    batches = minibatch(train_data, size=8)\n",
    "    # Iterate through minibatches\n",
    "    for batch in batches:\n",
    "        for text, labels in batch:\n",
    "            doc = nlp.make_doc(text)\n",
    "            example = Example.from_dict(doc, labels)\n",
    "            nlp.update([example], sgd=optimizer, losses=losses)\n",
    "    print(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    \"\"\"Multi class version of Logarithmic Loss metric.\n",
    "    :param actual: Array containing the actual target classes\n",
    "    :param predicted: Matrix with class predictions, one probability per class\n",
    "    \"\"\"\n",
    "    # Convert 'actual' to a binary array if it's not already:\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "\n",
    "    clip = np.clip(predicted, eps, 1 - eps)\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(actual * np.log(clip))\n",
    "    return -1.0 / rows * vsota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(668, 5)\n"
     ]
    }
   ],
   "source": [
    "docs = [nlp.tokenizer(text) for text in X_test['text']]\n",
    "\n",
    "# use Textcat to get the scores for each doc\n",
    "textcat = nlp.get_pipe('textcat')\n",
    "scores = textcat.predict(docs)\n",
    "print(scores.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores are used to predict a single class or label by choosing the label with the highest probability. You get the index of the highest probability with scores.argmax, then use the index to get the label string from textcat.labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the scores, find the label with the highest score/probability\n",
    "predicted_labels = scores.argmax(axis=1)\n",
    "# predicted_labels\n",
    "predictions = np.array([textcat.labels[label] for label in predicted_labels]).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 business\n",
      "1 tech\n",
      "2 politics\n",
      "3 entertainment\n",
      "4 sport\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print (i, textcat.labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.replace({'business': 0, 'tech': 1, 'politics': 2, 'entertainment': 3, 'sport': 4}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss 0.410\n"
     ]
    }
   ],
   "source": [
    "print('logloss %0.3f' %multiclass_logloss(y_test, scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/06_models/nlp_model.joblib']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "# saving our model # model - model , filename-model_jlib\n",
    "joblib.dump(optimizer , '../data/06_models/nlp_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "1. Encoding\n",
    "2. Source : https://www.kaggle.com/code/abhishek/approaching-almost-any-nlp-problem-on-kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the LabelEncoder from scikit-learn to convert text labels to integers, 0, 1 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "all_texts = pd.read_csv('../data/02_intermediate/all_text_labels.csv')\n",
    "all_texts = all_texts.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "X = all_texts.drop('label',axis=1)\n",
    "y = all_texts['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "y = lbl_enc.fit_transform(y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split to get train and validation\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(X_train.text.values, y, \n",
    "                                                  stratify=y, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1401,)\n",
      "(156,)\n"
     ]
    }
   ],
   "source": [
    "print (xtrain.shape)\n",
    "print (xvalid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Model (TF_IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always start with these features. They work (almost) everytime!\n",
    "tfv = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')\n",
    "\n",
    "# Fitting TF-IDF to both training and test sets (semi-supervised learning)\n",
    "tfv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_tfv =  tfv.transform(xtrain) \n",
    "xvalid_tfv = tfv.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.446 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Logistic Regression on TFIDF\n",
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(xtrain_tfv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count-Vectorizer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), stop_words = 'english')\n",
    "\n",
    "# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\n",
    "ctv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_ctv =  ctv.transform(xtrain) \n",
    "xvalid_ctv = ctv.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.073 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Logistic Regression on Counts\n",
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(xtrain_ctv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_ctv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compared to TF-IDF (0.446) and Count Vectoriser (0.073), Logistic regression has improved a lot the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on XgBoost\n",
    " use CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\1. Data Science\\2. 2022\\[APR22] HACKATHON\\hack3.9\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:35:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "logloss: 0.080 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple xgboost on tf-idf\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf.fit(xtrain_ctv.tocsc(), ytrain)\n",
    "predictions = clf.predict_proba(xvalid_ctv.tocsc())\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems really good XGBoost Classifier (0.080), slightly worse that the Logistic regression of Count vectoriser.\n",
    "- may consider Logistic regression over this, since XGB model is slightly complex, compared to logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cfb64e94590a84ac1c013de56654628e36f564001618a7025de8c4fbe59cc576"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
